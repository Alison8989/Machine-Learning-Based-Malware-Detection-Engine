# -*- coding: utf-8 -*-
#!/usr/bin/python

from __future__ import print_function
from __future__ import division
import os
import random
import time
import pickle

import matplotlib.pyplot as plt
import numpy as np
import pefile
from sklearn import metrics

from PE_Classfy.feaExtractA import feature_gets
from vt_detect.sample_classfy_byVTAPI import baseInfor
from PE_Classfy.fresh_mal_db import fresh_DB_feature_get

DatabaseTrainNum_Normal = 1500
DatabaseTrainNum_Melidious = 2500
DatabaseTestNum_normal = 1500
DatabaseTestNum_Melidious = 1500

PEfile_normal_Path = r'/home/ldy/download/benign'
PEfile_melicious_Path = r'/home/ldy/download/20131011003ndb'

DatabaseTrainNum = DatabaseTrainNum_Normal + DatabaseTrainNum_Melidious
DatabaseTestNum = DatabaseTestNum_normal + DatabaseTestNum_Melidious

def fakeFile_check(fpath):
    file_object = open(fpath, 'rb')
    try:
        e_magic = file_object.read(2)
    finally:
        file_object.close()
    if (e_magic == b'MZ'):
        return True
    else:
        return False


def load_file(PEfile_Path):
    test_files = list()
    for dirpath, dirname, filenames in os.walk(PEfile_Path):
        for filename in filenames:
            rfpath = os.path.join(dirpath, filename)
            if (fakeFile_check(rfpath)):  # 只读取DOS文件
                test_files.append(rfpath)
    return test_files


def load_malware_file(PEfile_Path):
    test_files = list()
    for dirpath, dirname, filenames in os.walk(PEfile_Path):
        for filename in filenames:
            rfpath = os.path.join(dirpath, filename)
            if (fakeFile_check(rfpath)):  # 只读取DOS文件
                test_files.append(rfpath)
    return test_files

def datebase_divide():
    templist = [i for i in range(DatabaseTrainNum_Normal + DatabaseTestNum_normal)]
    trainlist_normal = random.sample(templist, DatabaseTrainNum_Normal)  # 选择正常的训练样本
    testlist_normal = [i for i in templist if i not in trainlist_normal]

    templist = [i + DatabaseTrainNum_Normal + DatabaseTestNum_normal for i in
                range(DatabaseTrainNum_Melidious + DatabaseTestNum_Melidious)]
    trainlist_melicious = random.sample(templist, DatabaseTrainNum_Melidious)
    testlist_melicious = [i for i in templist if i not in trainlist_melicious]

    trainlist = trainlist_normal + trainlist_melicious
    random.shuffle(trainlist)  # 训练样本随机性

    testlist = testlist_normal + testlist_melicious
    return trainlist, testlist

def GetFileName(filepath):
    filepath = filepath.split('/')
    return filepath[-1]

def isTragen(filepath,binf):
    species = binf.GetSpeciesByFileName(GetFileName(filepath))
    if species:
        if 'Trojan' in species:
            return 2
    return 1

TragenTrainNum = 0
TragenTestNum = 0
gTestFileNamleSet = list()

def data_get_ex(flag):  #读去指定的数据集
    global DatabaseTestNum_Melidious,DatabaseTestNum_normal,DatabaseTrainNum_Melidious,DatabaseTrainNum_Normal
    if flag:
        gcroot = load_file(PEfile_normal_Path)
        norm_num = DatabaseTrainNum_Normal + DatabaseTestNum_normal
    else:
        gcroot = load_malware_file(PEfile_melicious_Path)
        norm_num = DatabaseTrainNum_Melidious + DatabaseTestNum_Melidious

    dataset_feat = []
    i = 0
    allfilenum = len(gcroot)
    templist = [ig for ig in range(allfilenum)]
    trainlist_normal = random.sample(templist, norm_num + 100)  # 选择正常的训练样本
    while(True):
        if len(dataset_feat)< norm_num and i < allfilenum:
            try:
                file_root = gcroot[trainlist_normal[i]]
                p_files = pefile.PE(file_root, True)
                dataset_feat.append(feature_gets(p_files, file_root))  # 获取特征集
                p_files.close()
                i += 1
                print(i)
            except pefile.PEFormatError:
                i+= 1
                continue
        else:
            break
    if len(dataset_feat) != norm_num:
        print('warning:the lack of samples')
        return None
    else:
        return dataset_feat


def data_get(trainSampleMark, testSampleMark):  # 获取训练数据和测试数据
    global TragenTrainNum,TragenTestNum
    global  gTestFileNamleSet
    train_feature = []
    train_class = []
    test_feature = []
    test_class = []
    gcroot_normal = load_file(PEfile_normal_Path)
    gcroot_melicious = load_malware_file(PEfile_melicious_Path)

    print("normal sample number is %d" % len(gcroot_normal))
    print("malware sample number is %d" % len(gcroot_melicious))

    binf = baseInfor(r'/home/ldy/download/databaseinf2.pkl')

    for i in trainSampleMark:
        if (i < (DatabaseTrainNum_Normal + DatabaseTestNum_normal)):
            try:
                p_files = pefile.PE(gcroot_normal[i], True)
                train_class.append(0)
                train_feature.append(feature_gets(p_files,gcroot_normal[i]))  # 获取特征集
                p_files.close()
            except pefile.PEFormatError:
                continue
        else:
            try:
                p_files = pefile.PE(gcroot_melicious[i - (DatabaseTrainNum_Normal + DatabaseTestNum_normal)], True)
                train_feature.append(feature_gets(p_files,gcroot_melicious[i - (DatabaseTrainNum_Normal + DatabaseTestNum_normal)]))  # 获取特征集
                #三分类新增代码
                #jret = isTragen(gcroot_melicious[i - (DatabaseTrainNum_Normal + DatabaseTestNum_normal)],binf)
                #if jret == 2:
                   # TragenTrainNum += 1
                #train_class.append(jret)
                 #二分类代码
                train_class.append(1)
                p_files.close()
            except pefile.PEFormatError:
                continue
        print(i)
                # train_feature.append(feature_get_by_jiangwei(p_files))
    for i in testSampleMark:
        if (i < (DatabaseTrainNum_Normal + DatabaseTestNum_normal)):
            try:
                t_files = pefile.PE(gcroot_normal[i], True)
                test_feature.append(feature_gets(t_files,gcroot_normal[i]))
                test_class.append(0)
                t_files.close()
                gTestFileNamleSet.append(gcroot_normal[i])
            except pefile.PEFormatError:
                continue
        else:
            # print('testerror in [%d] -----[%s]' % (i, gcroot_melicious[i - (DatabaseTrainNum_Normal + DatabaseTestNum_normal)]))
            try:
                t_files = pefile.PE(gcroot_melicious[i - (DatabaseTrainNum_Normal + DatabaseTestNum_normal)], True)
                test_feature.append(feature_gets(t_files,gcroot_melicious[i - (DatabaseTrainNum_Normal + DatabaseTestNum_normal)]))

                # 三分类新增代码
                #jret = isTragen(gcroot_melicious[i - (DatabaseTrainNum_Normal + DatabaseTestNum_normal)],binf)
                #if jret == 2:
                 #  TragenTestNum += 1
                #test_class.append(jret)

                # 二分类代码
                test_class.append(1)
                t_files.close()
                gTestFileNamleSet.append(GetFileName(gcroot_melicious[i - (DatabaseTrainNum_Normal + DatabaseTestNum_normal)]))
            except pefile.PEFormatError:
                continue
        print(i)
                # test_feature.append(feature_get_by_jiangwei(t_files))
    return train_feature, train_class, test_feature, test_class


# Multinomial Naive Bayes Classifier
def naive_bayes_classifier(train_x, train_y):
    from sklearn.naive_bayes import MultinomialNB
    model = MultinomialNB(alpha=0.01)
    model.fit(train_x, train_y)
    return model


# KNN Classifier
def knn_classifier(train_x, train_y):
    from sklearn.neighbors import KNeighborsClassifier
    model = KNeighborsClassifier()
    model.fit(train_x, train_y)
    return model


# Logistic Regression Classifier
def logistic_regression_classifier(train_x, train_y):
    from sklearn.linear_model import LogisticRegression
    model = LogisticRegression(penalty='l2')
    model.fit(train_x, train_y)
    return model


# Random Forest Classifier
def random_forest_classifier(train_x, train_y,mal_flag = False):
    import math
    n_trees = 255
    bblen = len(train_x[0])
    max_feat = int(math.ceil(math.sqrt(bblen)*3))


    from sklearn.ensemble import RandomForestClassifier
    model = RandomForestClassifier(n_estimators=n_trees,max_features = 0.3,oob_score=True)
    model.fit(train_x, train_y)
    return model


# Decision Tree Classifier
def decision_tree_classifier(train_x, train_y):
    from sklearn import tree
    model = tree.DecisionTreeClassifier()
    model.fit(train_x, train_y)
    return model


# GBDT(Gradient Boosting Decision Tree) Classifier
def gradient_boosting_classifier(train_x, train_y):
    from sklearn.ensemble import GradientBoostingClassifier
    model = GradientBoostingClassifier(n_estimators=200)
    model.fit(train_x, train_y)
    return model


# SVM Classifier
def svm_classifier(train_x, train_y):
    from sklearn.svm import SVC
    model = SVC(kernel='rbf', probability=True)
    model.fit(train_x, train_y)
    return model


# SVM Classifier using cross validation
def svm_cross_validation(train_x, train_y):
    from sklearn.grid_search import GridSearchCV
    from sklearn.svm import SVC
    model = SVC(kernel='rbf', probability=True)
    param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001]}
    grid_search = GridSearchCV(model, param_grid, n_jobs=1, verbose=1)
    grid_search.fit(train_x, train_y)
    best_parameters = grid_search.best_estimator_.get_params()
    for para, val in best_parameters.items():
        print(para, val)
    model = SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True)
    model.fit(train_x, train_y)
    return model



def plot_importance(feature_importance):
    important_features = [str(i) for i in range(len(feature_importance))]
    feature_importance = 100.0 * (feature_importance / feature_importance.max())

    #调试使用
    saveFile(r'/home/ldy/download/featureimportance.pkl',feature_importance.tolist())

    important_features = np.array(important_features)
    sorted_idx = np.argsort(-feature_importance)
    print(sorted_idx)

    pos = np.arange(len(sorted_idx)) + 0.5
    plt.subplot(1, 2, 2)
    plt.title('Feature Importance')

    plt.barh(pos[0:30], feature_importance[sorted_idx][0:30], color='r', align='center')
    plt.yticks(pos[0:30], important_features[sorted_idx][0:30])
    plt.xlabel('Relative Importance')
    plt.draw()
    plt.show()

def plotimportance():
    f = open(r'/home/ldy/download/featureimportance.pkl','rb')
    feature_importance = pickle.load(f)
    f.close()
    fim = np.array(feature_importance)
    important_features=['DebugInfoVirturalAddress',
                        'DebugInfoSize',
                        'DLLCHARACTERISTICS_TerminalService',
                        'DLLCHARACTERISTICS_DYNAMIC_BASE',
                        'CertificateTableSize',
                        'RT_maniFest',
                        'MajorLinkVersion',
                        'NumberOfSymbols',
                        'DLLCHARACTERISTICS_TERMINAL_DEP',
                        'GetCurrentAddress&_initterm&UnhandleExceptionFilter',
                        'checksum=0',
                        'ExceptionTableSize',
                        'RT_Cursor',
                        'Sleep&_initterm&UnhandleExceptionFilter',
                        '.text SizeofRawData',
                        'number of APIs',
                        'majorSubSystemVersion',
                        'resourceTableSize',
                        'minorSubSystemVersion',
                        'section write&excute',
                        'Sleep&_initterm&GettickCount'
                        ]
    important_features = np.array(important_features)
    sorted_idx = np.argsort(-fim)

    pos = np.arange(len(sorted_idx)) + 0.5
    plt.subplot(1, 2, 2)
    plt.title('Feature Importance')

    plt.barh(pos[0:21], fim[sorted_idx][0:21], color='r', align='center')
    plt.yticks(pos[0:21], important_features[0:21])
    plt.xlabel('Relative Importance')
    plt.draw()
    plt.show()

def plotROC(probas_,test_y,label = 1):
    from scipy import interp
    from sklearn.metrics import roc_curve, auc

    ###############################################################################
    #ROC analysis
    # 分类，做ROC分析

    mean_tpr = 0.0
    mean_fpr = np.linspace(0, 1, 100)

    # Compute ROC curve and area the curve
    # 通过roc_curve()函数，求出fpr和tpr，以及阈值
    fpr, tpr, thresholds = roc_curve(test_y, probas_[:,1],pos_label=label)
    mean_tpr += interp(mean_fpr, fpr, tpr)  # 对mean_tpr在mean_fpr处进行插值，通过scipy包调用interp()函数
    mean_tpr[0] = 0.0  # 初始处为0
    roc_auc = auc(fpr, tpr)
    # 画图，只需要plt.plot(fpr,tpr),变量roc_auc只是记录auc的值，通过auc()函数能计算出来
    plt.plot(fpr, tpr, lw=1, label='ROC  area = %0.2f)' % (roc_auc))

    # 画对角线
    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')

    plt.xlim([-0.05, 1.05])
    plt.ylim([-0.05, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC curve ')
    plt.legend(loc="lower right")
    plt.show()

def RF_Information_print(clf):
    plot_importance(clf.feature_importances_)

def fdata_ZOO_get():
    import zooTest
    zooTest.printInfAboutTheZoo()
    filePath,malFamilly = zooTest.getZooFilePath()
    test_feature = []
    test_class = []
    for i in range(len(filePath)):
        try:
            t_files = pefile.PE(filePath[i], True)
            test_feature.append(feature_gets(t_files))
            test_class.append(1)
        except pefile.PEFormatError:
            continue
        finally:
            t_files.close()
            os.remove(filePath[i])
    return  test_feature,test_class

def predect_calcu(predict,test_y,binary_class):
    if binary_class:
        precision = metrics.precision_score(test_y, predict)
        recall = metrics.recall_score(test_y, predict)

        confusion = metrics.confusion_matrix(test_y, predict)
        TP = confusion[1, 1]
        TN = confusion[0, 0]
        FP = confusion[0, 1]
        FN = confusion[1, 0]
        print("TP:%d *** TN:%d *** FP:%d *** FN:%d " % (TP, TN, FP, FN))
        print('precision: %.2f%%, recall: %.2f%%' % (100 * precision, 100 * recall))
    accuracy = metrics.accuracy_score(test_y, predict)
    print('accuracy: %.2f%%' % (100 * accuracy))
    return accuracy,confusion


def mal_count(feature,cla):
    lens = len(feature[0])
    #均值
    norm_count = [0 for i in range(lens)]
    trojan_count = [0 for i in range(lens)]
    mal_counts = [0 for i in range(lens)]
    tnums = 0
    mnums = 0
    nnums = 0
    for i in range(len(cla)):
        if cla[i] ==1:
            mnums += 1
            for k in range(lens):
                mal_counts[k] = mal_counts[k] + (feature[i][k] - mal_counts[k]) / mnums
        elif cla[i] == 2:
            tnums += 1
            for k in range(lens):
                trojan_count[k] = trojan_count[k] + (feature[i][k]-trojan_count[k]) / tnums
        elif cla[i] == 0:
            nnums += 1
            for k in range(lens):
                norm_count[k] = norm_count[k] + (feature[i][k]- norm_count[k]) / nnums
    #方差
    trojan_th_ta = [0 for i in range(lens)]
    mal_th_ta = [0 for i in range(lens)]
    norm_th_ta = [0 for i in range(lens)]
    for i in range(len(cla)):
        if cla[i] ==1:
            for k in range(lens):
                mal_th_ta[k] += ((feature[i][k] - mal_counts[k]) * (feature[i][k] - mal_counts[k]))
        elif cla[i] == 2:
            for k in range(lens):
                trojan_th_ta[k] += ((feature[i][k] - trojan_count[k]) * (feature[i][k] - trojan_count[k]))
        elif cla[i] == 0:
            for k in range(lens):
                norm_th_ta[k] += ((feature[i][k] - norm_count[k]) * (feature[i][k] - norm_count[k]))
    import math
    for k in range(lens):
        if mnums != 0:
            mal_th_ta[k] = math.sqrt(mal_th_ta[k] / mnums)
        if tnums != 0:
            trojan_th_ta[k] = math.sqrt(trojan_th_ta[k] / tnums)
        if nnums != 0:
            norm_th_ta[k] = math.sqrt(norm_th_ta[k]/nnums)

    print('  mal          ','        trojan      ')
    for i in range(lens):
        print('%-13.3f\t%-13.3f\t %-13.3f\t%-13.3f\t %-13.3f\t%-13.3f'%(norm_count[i],norm_th_ta[i],mal_counts[i],mal_th_ta[i],trojan_count[i],trojan_th_ta[i]))

def ClassfyDetailInfor(predict_prob,test_y): #获取分类的中间概率信息
    global gTestFileNamleSet
    TP_Rate_Count = [0,0,0,0,0,0]
    TN_Rate_Count = [0,0,0,0,0,0]
    F_count={}
    def Acc(rate,count):
        if rate < 0.5:
            count[0] += 1
        elif rate>=0.5 and rate < 0.6:
            count[1] += 1
        elif rate >= 0.6 and rate < 0.7:
            count[2] += 1
        elif rate>=0.7 and rate < 0.8:
            count[3] += 1
        elif rate>=0.8 and rate < 0.9:
            count[4] += 1
        elif rate>=0.9 and rate <=1:
            count[5] += 1
    for i in range(len(predict_prob)):
        prob = predict_prob[i].tolist()
        g = max(prob)
        index = prob.index(g)
        if(index == test_y[i]):
            if(index == 0):
                Acc(g,TN_Rate_Count)
            else:
                Acc(g,TP_Rate_Count)
        else:
            pass
            #F_count[gTestFileNamleSet[i]]=[test_y[i],index,prob]  #[实际类别，错误预测类别，各类的预测概率]
    return TP_Rate_Count,TN_Rate_Count,F_count

def innerDetaiInforGet(predictp, test_y):
    aa, bb, cc = ClassfyDetailInfor(predictp, test_y)
    print('TP:  ',aa)
    print('TN:  ',bb)
    gbcount = 0
    iccount = 0
    acount = 0
    bcount = 0
    ccount = 0
    sample = [i for i in range(len(cc))]
    if len(sample) < 101:
        rsample = sample
    else:
        rsample = random.sample(sample, 100)  # 打印前100条信息
    for (k, v) in cc.items():
        if v[0] == 0: #正常被分类错误的个数
            acount += 1
        elif v[0] == 1:  # 病毒被分类错误的个数
            bcount += 1
        else:  # 木马被分类错误的个数
            ccount += 1
        if v[0] == 2 or v[1] == 2:
            iccount += 1
        if gbcount in rsample:
            print(k, '           ', v)
        gbcount += 1
    print('normal error',acount)
    print('mal error', bcount)
    print('trojan error', ccount)
    print('tragen error :', iccount)



def malware_sample_training_RF(train_x,train_y):
    '''
    :param train_x: 训练特征集
    :param train_y: 包含有多个类别
    :return: 分类模型
    '''
    global TragenTrainNum
    malcount = 0
    mal_train_x =[]
    mal_train_y = []
    nums = len(train_y)
    li_train_y = [0 for i in range(nums)]
    for i in range(nums):
        if train_y[i] > 0:
            li_train_y[i] = 1

            #第一种训练：提取病毒特征单独训练，不要求数量相等
            #mal_train_x.append(train_x[i])
            #mal_train_y.append(train_y[i])

            # 第二种训练：让木马样本的数量与其他病毒的数量相等
            #if (train_y[i] == 1 and malcount < TragenTrainNum) or train_y[i] == 2:
             #   mal_train_x.append(train_x[i])
              #  mal_train_y.append(train_y[i])
               # if train_y[i] == 1:
                #    malcount += 1

    modela = random_forest_classifier(train_x= train_x, train_y = li_train_y)

    # 第三种训练：在第一种训练的基础上，先进行特征进行选择
    #from sklearn.ensemble import RandomForestRegressor
    #rf = RandomForestRegressor(n_estimators = 100)
    #selectmodel = rf.fit(mal_train_x, mal_train_y)
    #select_x =selectmodel.transform(mal_train_x)

    #第四种训练：抽取API序列特征
    from PE_Classfy.API_feature_get import api_feature_get_ex
    select_x,mal_train_y,selectmodel = api_feature_get_ex(train_x,train_y,88,225)

    modelb = random_forest_classifier(train_x= select_x,train_y = mal_train_y,mal_flag= True)

    return modela,modelb,selectmodel

def mulClassfypPro_RF(modela,modelb,selec_model,test_x,num_class):

    #预测分类
    predict = modela.predict(test_x)  #二分类
    predictdb = [it for it in predict]
    temp_sample =[]
    ind = []
    for i in range(len(predict)):
        if predict[i] >0:#malware classification
            temp_sample.append(test_x[i])
            ind.append(i)
    #第三种训练方法对应的特征测试算法，特征选择
    #predictb = modelb.predict(selec_model.transform(temp_sample))
    #第四种训练方法，对应的测试算法，特征转换，API序列提取
    from PE_Classfy.API_feature_get import feature_transform_ex
    predictb = modelb.predict(feature_transform_ex(temp_sample,88,225,selec_model))
    ic = 0
    for index in ind:
        predict[index] = predictb[ic]
        ic += 1

    del temp_sample,ind

    #预测分类概率
    predictp = modela.predict_proba(test_x)
    predictp = predictp.tolist()
    temp_samples =[]
    inds = []
    for i in range(len(predictp)):
        if len(predictp[i]) != 2:
            ggggg = 0
        if predictp[i][0] < predictp[i][1]:#malware classification,throld
            temp_samples.append(test_x[i])
            inds.append(i)
    if temp_samples:
        # 第三种训练方法对应的特征测试算法，特征选择
        #predictg = modelb.predict_proba(selec_model.transform(temp_samples))
        # 第四种训练方法，对应的测试算法，特征转换，API序列提取
        predictg = modelb.predict_proba(feature_transform_ex(temp_samples,88,225,selec_model))
        predictg = predictg.tolist()
    ic = 0
    for i in range(len(predictp)):
        temp = predictp[i]
        if i in inds:
            btemp = predictg[ic]
            acc = [0]
            for igd in range(len(btemp)):
                acc.append(btemp[igd])
            predictp[i] = acc
            ic += 1
        else:
            temp.extend([0 for ib in range(num_class - 2)])
            predictp[i] = temp

    return predict,predictp,predictdb,

def malware_sample_training_RF_ex(train_x,train_y):

    from PE_Classfy.API_feature_get import api_feature_get_ex
    li_train_y = []
    for it in train_y:
        if it > 0:
            li_train_y.append(2)
        else:
            li_train_y.append(1)
    select_x,mal_train_y,selectmodel = api_feature_get_ex(train_x,li_train_y,88,225)

    modelb = random_forest_classifier(train_x= select_x,train_y = mal_train_y,mal_flag= True)

    return modelb,selectmodel

def mulClassfypPro_RF_ex(modela,modelb,selec_model,test_x):

    #预测分类
    predict = modela.predict(test_x)  #二分类

    #三分类
    from PE_Classfy.API_feature_get import feature_transform_ex
    predictb = modelb.predict(feature_transform_ex(test_x,88,225,selec_model))
    gpredictb = [i-1 for i in predictb]

    return predict,gpredictb


def RF_param_select(train_x,train_y,test_x,test_y,tacc):
    from sklearn.ensemble import RandomForestClassifier
    ntrees = (200,250,300)
    split = (1,2,4)
    max_feat = (0.2,0.3,0.4)
    obb =  (True,False)
    ij = 0
    for maxf in max_feat:
        for sp in split:
            for ob in obb:
                for ntree in ntrees:
                    print('classfy by RF')
                    model = RandomForestClassifier(n_estimators=ntree, max_features=maxf,oob_score=ob,min_samples_split=sp)
                    model.fit(train_x, train_y)
                    predict = model.predict(test_x)
                    acc, confusion = predect_calcu(predict, test_y, True)
                    tacc[ij] += acc
                    ij += 1

def plot_RF_parame(tacc):
    import math
    for ibdd in range(54):
        tacc[ibdd] = tacc[ibdd] / 1

    imp = np.array(tacc)

    sorted_idx = np.argsort(-imp)
    bt = [sorted_idx[0]]
    gt = [math.floor(sorted_idx[0]/3)]

    i = 1
    while(1):
        if len(bt) < 3:
            tmp = math.floor(sorted_idx[i]/3)
            if tmp not in gt:
                gt.append(tmp)
                bt.append(sorted_idx[i])
            i += 1
        else:
            break

    #绘图
    x = [200,250,300]
    split = ('1,','2,','4,')
    max_feat = ('0.2,','0.3,','0.4,')
    obb =  ('True','False')
    colo = ['r','g','b']
    ij = 0
    tt = 0
    plt.figure()
    plt.xlabel('n_trees')
    plt.ylabel('accuracy')
    plt.ylim(0.99,1)
    plt.title('RF accuracy')
    for maxf in max_feat:
        strss = 'maxfeat='
        strss += maxf
        for sp in split:
            strg='maxsplit='
            strg += sp
            for ob in obb:
                strb ='obbscore='
                strb+= ob
                for tr in x:
                    if ij in bt:
                        strs = (strss+ strg + strb)
                        tmp = int(math.floor(ij / 3))
                        #plt.plot(x,[tacc[tmp*3],tacc[tmp*3+1],tacc[tmp*3+2]])
                        plt.plot(x, [tacc[tmp*3],tacc[tmp*3+1],tacc[tmp*3+2]], label='$'+strs+'$', color=colo[tt],
                                 linewidth=2)
                        tt+=1
                    ij += 1
    plt.legend()
    plt.show()


def saveFile(groot,data):
    f = open(groot, 'wb')
    pik = pickle.Pickler(f)
    if data:
        pik.dump(data)
    f.close()

def cross_classfy(old_class):
    import math
    print('start processing!')

    #test_classifiers = ['NB', 'KNN', 'LR', 'RF', 'DT','SVM','GBDT']
    test_classifiers = ['NB', 'KNN', 'LR', 'RF', 'SVM']
    classifiers = {'NB': naive_bayes_classifier,
                   'KNN': knn_classifier,
                   'LR': logistic_regression_classifier,
                   'RF': random_forest_classifier,
                   'DT': decision_tree_classifier,
                   'SVM': svm_classifier,
                   'SVMCV': svm_cross_validation,
                   'GBDT': gradient_boosting_classifier
                   }

    def classfy(train_x,train_y,test_x,test_y,i,mechine):
        global classifier
        is_binary_class = True
        model = classifiers[mechine](train_x, train_y)
        predict = model.predict(test_x)
        if (mechine == 'RF' and i == 0):  # 打印随机森林的分类详细信息
            # 中间的分类概率信息
            RF_Information_print(model)

        acc, confusion = predect_calcu(predict, test_y, is_binary_class)
        return acc,confusion
    normroot = r'/home/ldy/download/databaseFeatNorm.pkl'
    malroot = r'/home/ldy/download/databaseFeatmal.pkl'

    if old_class and os.path.exists(normroot):
        f = open(normroot,'rb')
        norm_feat = pickle.load(f)
        f.close()
    else:
        norm_feat = data_get_ex(True)
        saveFile(normroot,norm_feat)

    if old_class and os.path.exists(malroot):
        f = open(malroot,'rb')
        mal_feat = pickle.load(f)
        f.close()
    else:
        mal_feat = data_get_ex(False)
        saveFile(malroot, mal_feat)

    num_norm = len(norm_feat)
    num_mal = len(mal_feat)

    all_sample_num = num_norm + num_mal
    train_per_norm = math.ceil(num_norm * 0.5)
    train_per_mal = math.ceil(num_mal * 0.5)

    api_start = 68
    api_end = 205
    #十折交叉验证
    from PE_Classfy.API_feature_get import api_feature_get
    from PE_Classfy.API_feature_get import feature_transform
    bdict = {}
    tacc = [0 for i in range(54)]
    for i in range(1):
        #获取训练样本和测试样本
        train_sample = [ic for ic in range(num_norm) if ic < (i * train_per_norm) or ic >= ((i+1)* train_per_norm)]

        train_sample.extend([ic + num_norm for ic in range(num_mal) if ic < (i * train_per_mal) or ic >= ((i+1)* train_per_mal)])
        tlen_norm = len(train_sample)
        templist = [gc for gc in range(tlen_norm)]
        gtemplist = random.sample(templist,tlen_norm)

        train_x = []
        train_y = []
        for index in gtemplist:
            if train_sample[index] < num_norm:
                train_x.append(norm_feat[train_sample[index]])
                train_y.append(0)
            else:
                train_x.append(mal_feat[train_sample[index] - num_norm])
                train_y.append(1)
        test_x = []
        test_y = []
        for ind in range(num_norm):
            if ind not in train_sample:
                test_x.append(norm_feat[ind])
                test_y.append(0)
        for ind in range(num_mal):
            gind = ind + num_norm
            if gind not in train_sample:
                test_x.append(mal_feat[ind])
                test_y.append(1)

        #api序列特征获取
        li_trainy = [temp+1 for temp in train_y]
        train_api_seq, train_mal_class, gresult_dict = api_feature_get(train_x, li_trainy,api_start,api_end)
        test_api_seq = []
        for ft in test_x:
            temp = feature_transform(ft[api_start:api_end], gresult_dict)
            test_api_seq.append(temp)

        del gresult_dict,train_mal_class

        #api特征获取,PE header特征获取
        train_pehead = []
        train_api = []
        feat_len = len(train_x[0])
        for ft in train_x:
            temp = ft[api_start:api_end]
            try:
                btemp = [ft[its] for its in range(feat_len) if its < api_start or its >= api_end]
                train_api.append(temp)
                train_pehead.append(btemp)
            except Exception:
                A = 0

        test_api = []
        test_peheand = []
        for ft in test_x:
            temp = ft[api_start:api_end]
            btemp = [ft[its] for its in range(feat_len) if its < api_start or its >= api_end]
            test_api.append(temp)
            test_peheand.append(btemp)

        #all
        igg = 0
        for ft in train_x:
            ft.extend(train_api_seq[igg])
            igg+=1
        igg = 0
        for ft in test_x:
            ft.extend(test_api_seq[igg])
            igg += 1

        #plotROC(train_x,train_y,test_x,test_y)
        param = False
        for classifier in test_classifiers:
            if param:
                print('******************* %s%d ********************' % (classifier,i))
                acc, confusion = classfy(train_api_seq, train_y, test_api_seq, test_y, 10, classifier)  # api seq
                TP = confusion[1, 1]
                TN = confusion[0, 0]
                FP = confusion[0, 1]
                FN = confusion[1, 0]
                bdict[classifier + 'apiseq' + str(i)] = [acc, TP, TN, FP, FN]

                acc, confusion = classfy(train_api, train_y, test_api, test_y, 10, classifier)  # api
                TP = confusion[1, 1]
                TN = confusion[0, 0]
                FP = confusion[0, 1]
                FN = confusion[1, 0]
                bdict[classifier + 'api' + str(i)] = [acc, TP, TN, FP, FN]

                acc, confusion = classfy(train_pehead, train_y, test_peheand, test_y, 10, classifier)  # pe
                TP = confusion[1, 1]
                TN = confusion[0, 0]
                FP = confusion[0, 1]
                FN = confusion[1, 0]
                bdict[classifier + 'pehead' + str(i)] = [acc, TP, TN, FP, FN]

                acc, confusion = classfy(train_x, train_y, test_x, test_y, i, classifier)  # all
                TP = confusion[1, 1]
                TN = confusion[0, 0]
                FP = confusion[0, 1]
                FN = confusion[1, 0]
                bdict[classifier + 'all' + str(i)] = [acc, TP, TN, FP, FN]
            else:
                if classifier == 'RF':
                    RF_param_select(train_x, train_y, test_x, test_y, tacc)
                    print(tacc)
                    plot_RF_parame(tacc)
    if param:
        gb = ('apiseq', 'api', 'pehead', 'all')
        for classifier in test_classifiers:
            for gb_it in gb:
                acc = 0
                for id in range(10):
                    key = classifier + gb_it + str(id)
                    acc += bdict[key][0]
                print(key, ' acc:', (acc / 10))
    return 0

def main():
    print('start processing!')
    trainSampleMark, testSampleMark = datebase_divide()

    thresh = 0.5
    model_save_file = None
    model_save = {}

    test_classifiers = ['NB', 'KNN', 'LR', 'RF', 'DT', 'GBDT']
    classifiers = {'NB': naive_bayes_classifier,
                   'KNN': knn_classifier,
                   'LR': logistic_regression_classifier,
                   'RF': random_forest_classifier,
                   'DT': decision_tree_classifier,
                   #'SVM': svm_classifier,
                   'SVMCV': svm_cross_validation,
                   'GBDT': gradient_boosting_classifier
                   }
    print('reading training and testing data...')

    #ztest_x,ztest_y = fdata_ZOO_get()
    ftest_x = []
    ftest_y = []
    #ftest_x,ftest_y = fresh_DB_feature_get()


    train_x, train_y, test_x, test_y = data_get(trainSampleMark, testSampleMark)


    #一下为特征统计
    #mal_count(train_x+test_x,train_y+test_y)
    #print('fresh db')
    #mal_count(ftest_x,ftest_y)
    #return


    num_train = len(train_x)
    num_feat = len(train_x[0])
    num_test = len(test_x)
    znum_test = 0  # len(ztest_y)
    fnum_test = len(ftest_y)

    num_class = len(np.unique(train_y))
    is_binary_class = (num_class == 2)
    print('******************** Data Info *********************')
    print( '#training data: %d, #testing_data: %d, dimension: %d' % (num_train, num_test, num_feat))

    for classifier in test_classifiers:
        print('******************* %s ********************' % classifier)
        start_time = time.time()
        if (classifier == 'RF'):  # 打印随机森林的部分信息
            modela,modelb,selectmode = malware_sample_training_RF(train_x,train_y)

        else:
            model = classifiers[classifier](train_x, train_y)

        print('training took %fs!' % (time.time() - start_time))


        if (classifier == 'RF'):  # 打印随机森林的分类详细信息
            predict,predictp,predictdb = mulClassfypPro_RF(modela, modelb,selectmode, test_x, num_class)
            # 中间的分类概率信息
            innerDetaiInforGet(predictp, test_y)
        else:
            predict = model.predict(test_x)

        if znum_test != 0:
            pass
            #if (classifier == 'RF'):  # 打印随机森林的分类详细信息
            #   predict_zoo = modela.predict(ztest_x)  # the zoo
            #else:
            #    predict_zoo = model.predict(ztest_x)  # the zoo

        if fnum_test != 0:
            if (classifier == 'RF'):  # 打印随机森林的分类详细信息
                #第二种方法
                modelh, selectmodeg = malware_sample_training_RF_ex(train_x, train_y)
                predict_f,predict_g = mulClassfypPro_RF_ex(modela, modelh,selectmodeg, ftest_x)
                #第一种方法
                #predict_f = modela.predict(ftest_x)
            else:
                predict_f = model.predict(ftest_x)


        if model_save_file != None:
            model_save[classifier] = model
        print('db3 database')
        predect_calcu(predict,test_y,is_binary_class)
        if (classifier == 'RF'):  # 打印随机森林的分类详细信息
            itm = [it for it in test_y]
            for i in range(len(itm)):
                if itm[i] > 0:
                    itm[i] = 1
            print('the li classfy result')
            predect_calcu(predictdb, itm, True)
            #RF_Information_print(modela)
            #RF_Information_print(modelb)

        #print('the zoo database')
        #predect_calcu(predict_zoo, ztest_y,True)
        if fnum_test != 0:
            print('the fresh DB to test')
            predect_calcu(predict_f,ftest_y,False)

            #api sequence 分类，第二种方法测试
            if (classifier == 'RF'):
                print('the fresh DB to test with API seq')
                predect_calcu(predict_g, ftest_y, False)


    print('Tragen train num. is ',TragenTrainNum)
    print('Tragen test num. is ', TragenTestNum)
    content = input("input:")
    #print("malware count information")
    #print(count_dic_malware)
    #print("denign count information")
    #print(count_dic_normal)
    return 0


if __name__ == '__main__':
   # main()
    #plotimportance()
    cross_classfy( old_class = True)